import numpy as np

class NeuralNetwork:
    def __init__(self, layers):
        self.layers = layers
        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]
        self.biases = [np.random.randn(1, layers[i+1]) for i in range(len(layers)-1)]

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def softmax(self, x):
        exps = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward(self, x):
        activations = [x]
        for i in range(len(self.layers) - 2):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            activations.append(self.sigmoid(z))
        
        # Output layer with softmax activation for multi-class classification
        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]
        output = self.softmax(z)
        
        return activations, output

# Example usage
if __name__ == '__main__':
    # Define a simple 3-layer neural network (2 input, 3 hidden, 2 output)
    layers = [2, 3, 2]
    neural_net = NeuralNetwork(layers)

    # Test input
    x = np.array([[1, 2]])

    # Forward pass
    activations, output = neural_net.forward(x)
    
    # Print activations and output
    print("Activations:")
    for layer_activation in activations:
        print(layer_activation)
    
    print("\nOutput:")
    print(output)
